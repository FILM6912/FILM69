{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "from cached_path import cached_path\n",
    "from FILM69.tts.f5_tts.model import CFM, UNetT, DiT, Trainer\n",
    "from FILM69.tts.f5_tts.model.utils import get_tokenizer\n",
    "from FILM69.tts.f5_tts.model.dataset import load_dataset\n",
    "from importlib.resources import files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args=dict(\n",
    "    exp_name='F5TTS_Base',\n",
    "    dataset_name='th',\n",
    "    learning_rate=1e-05,\n",
    "    batch_size_per_gpu=3200,\n",
    "    batch_size_type='frame',\n",
    "    max_samples=4,\n",
    "    grad_accumulation_steps=1,\n",
    "    max_grad_norm=1.0,\n",
    "    epochs=1000,\n",
    "    num_warmup_updates=0,\n",
    "    save_per_updates=3*500,\n",
    "    last_per_updates=3*500,\n",
    "    keep_last_n_checkpoints=-1,\n",
    "    finetune=True,\n",
    "    pretrain=None,\n",
    "    tokenizer='pinyin',\n",
    "    tokenizer_path=None,\n",
    "    log_samples=True,\n",
    "    logger='tensorboard',\n",
    "    bnb_optimizer=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"path_data\"]=\"/mnt/data/notebook/shared/FILM/research/F5/data\"\n",
    "os.environ[\"path_ckpts\"]=\"/mnt/data/notebook/shared/FILM/research/F5/ckpts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------- Dataset Settings --------------------------- #\n",
    "target_sample_rate = 24000\n",
    "n_mel_channels = 100\n",
    "hop_length = 256\n",
    "win_length = 1024\n",
    "n_fft = 1024\n",
    "mel_spec_type = \"vocos\"  # 'vocos' or 'bigvgan'\n",
    "\n",
    "def main():\n",
    "\n",
    "    checkpoint_path = f'{os.environ[\"path_ckpts\"]}/{args[\"dataset_name\"]}'\n",
    "\n",
    "    # Model parameters based on experiment name\n",
    "    if args[\"exp_name\"] == \"F5TTS_Base\":\n",
    "        wandb_resume_id = None\n",
    "        model_cls = DiT\n",
    "        model_cfg = dict(dim=1024, depth=22, heads=16, ff_mult=2, text_dim=512, conv_layers=4)\n",
    "        if args[\"finetune\"]:\n",
    "            if args[\"pretrain\"] is None:\n",
    "                ckpt_path = str(cached_path(\"hf://SWivid/F5-TTS/F5TTS_Base/model_1200000.pt\"))\n",
    "            else:\n",
    "                ckpt_path = args[\"pretrain\"]\n",
    "    elif args[\"exp_name\"] == \"E2TTS_Base\":\n",
    "        wandb_resume_id = None\n",
    "        model_cls = UNetT\n",
    "        model_cfg = dict(dim=1024, depth=24, heads=16, ff_mult=4)\n",
    "        if args[\"finetune\"]:\n",
    "            if args[\"pretrain\"] is None:\n",
    "                ckpt_path = str(cached_path(\"hf://SWivid/E2-TTS/E2TTS_Base/model_1200000.pt\"))\n",
    "            else:\n",
    "                ckpt_path = args[\"pretrain\"]\n",
    "\n",
    "    if args[\"finetune\"]:\n",
    "        if not os.path.isdir(checkpoint_path):\n",
    "            os.makedirs(checkpoint_path, exist_ok=True)\n",
    "\n",
    "        file_checkpoint = os.path.basename(ckpt_path)\n",
    "        if not file_checkpoint.startswith(\"pretrained_\"):  # Change: Add 'pretrained_' prefix to copied model\n",
    "            file_checkpoint = \"pretrained_\" + file_checkpoint\n",
    "        file_checkpoint = os.path.join(checkpoint_path, file_checkpoint)\n",
    "        if not os.path.isfile(file_checkpoint):\n",
    "            shutil.copy2(ckpt_path, file_checkpoint)\n",
    "            print(\"copy checkpoint for finetune\")\n",
    "\n",
    "    # Use the tokenizer and tokenizer_path provided in the command line arguments\n",
    "    tokenizer = args[\"tokenizer\"]\n",
    "    if tokenizer == \"custom\":\n",
    "        if not args[\"tokenizer_path\"]:\n",
    "            raise ValueError(\"Custom tokenizer selected, but no tokenizer_path provided.\")\n",
    "        tokenizer_path = args[\"tokenizer_path\"]\n",
    "    else:\n",
    "        tokenizer_path = args[\"dataset_name\"]\n",
    "\n",
    "    vocab_char_map, vocab_size = get_tokenizer(tokenizer_path, tokenizer)\n",
    "\n",
    "    print(\"\\nvocab : \", vocab_size)\n",
    "    print(\"\\nvocoder : \", mel_spec_type)\n",
    "\n",
    "    mel_spec_kwargs = dict(\n",
    "        n_fft=n_fft,\n",
    "        hop_length=hop_length,\n",
    "        win_length=win_length,\n",
    "        n_mel_channels=n_mel_channels,\n",
    "        target_sample_rate=target_sample_rate,\n",
    "        mel_spec_type=mel_spec_type,\n",
    "    )\n",
    "\n",
    "    model = CFM(\n",
    "        transformer=model_cls(**model_cfg, text_num_embeds=vocab_size, mel_dim=n_mel_channels),\n",
    "        mel_spec_kwargs=mel_spec_kwargs,\n",
    "        vocab_char_map=vocab_char_map,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model,\n",
    "        args[\"epochs\"],\n",
    "        args[\"learning_rate\"],\n",
    "        num_warmup_updates=args[\"num_warmup_updates\"],\n",
    "        save_per_updates=args[\"save_per_updates\"],\n",
    "        keep_last_n_checkpoints=args[\"keep_last_n_checkpoints\"],\n",
    "        checkpoint_path=checkpoint_path,\n",
    "        batch_size=args[\"batch_size_per_gpu\"],\n",
    "        batch_size_type=args[\"batch_size_type\"],\n",
    "        max_samples=args[\"max_samples\"],\n",
    "        grad_accumulation_steps=args[\"grad_accumulation_steps\"],\n",
    "        max_grad_norm=args[\"max_grad_norm\"],\n",
    "        logger=args[\"logger\"],\n",
    "        wandb_project=args[\"dataset_name\"],\n",
    "        wandb_run_name=args[\"exp_name\"],\n",
    "        wandb_resume_id=wandb_resume_id,\n",
    "        log_samples=args[\"log_samples\"],\n",
    "        last_per_updates=args[\"last_per_updates\"],\n",
    "        bnb_optimizer=args[\"bnb_optimizer\"],\n",
    "    )\n",
    "\n",
    "    train_dataset = load_dataset(args[\"dataset_name\"], tokenizer, mel_spec_kwargs=mel_spec_kwargs)\n",
    "\n",
    "    trainer.train(\n",
    "        train_dataset,\n",
    "        resumable_with_seed=666,  # seed for shuffling dataset\n",
    "    )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TTS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
